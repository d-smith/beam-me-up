When executing your pipeline with the DataflowRunner, you can use Stackdriver Logging. Stackdriver Logging aggregates the logs from all of your Cloud Dataflow job’s workers to a single location in the Google Cloud Platform Console. You can use Stackdriver Logging to search and access the logs from all of the workers that Cloud Dataflow has spun up to complete your job. Logging statements in your pipeline’s DoFn instances will appear in Stackdriver Logging as your pipeline runs.

You can also control the worker log levels. Cloud Dataflow workers that execute user code are configured to log to Stackdriver Logging by default at “INFO” log level and higher. You can override log levels for specific logging namespaces by specifying: --workerLogLevelOverrides={"Name1":"Level1","Name2":"Level2",...}. For example, by specifying --workerLogLevelOverrides={"org.apache.beam.examples":"DEBUG"} when executing a pipeline using the Cloud Dataflow service, Stackdriver Logging will contain only “DEBUG” or higher level logs for the package in addition to the default “INFO” or higher level logs.

The default Cloud Dataflow worker logging configuration can be overridden by specifying --defaultWorkerLogLevel=<one of TRACE, DEBUG, INFO, WARN, ERROR>. For example, by specifying --defaultWorkerLogLevel=DEBUG when executing a pipeline with the Cloud Dataflow service, Cloud Logging will contain all “DEBUG” or higher level logs. Note that changing the default worker log level to TRACE or DEBUG significantly increases the amount of logs output.

